{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.argv=['main.ipynb', '--config', 'configs/cifar_Fedavg.yaml','--data', './data','--mode','dirichlet','--dirichlet_alpha','1.5','--batch_size','50',\"--centralized_epochs\",'150',\"--global_epochs\",'3','--alpha','0.5','--epsilon','0.1','--momentum','0.9','--lr','0.1','--learning_rate_decay','0.992','--weight_decay','1e-3','--seed','0','--set','CIFAR10','--arch','CNN','--workers','8','--additional_experiment_name','','--alpha_divide_epoch']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Reading YAML config from configs/cifar_Fedavg.yaml\n"
     ]
    }
   ],
   "source": [
    "from args import args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No traceback available to show.\n"
     ]
    }
   ],
   "source": [
    "%tb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(additional_experiment_name='', alpha=0.5, alpha_divide_epoch=True, alpha_mul_epoch=False, arch='CNN', batch_size=50, centralized_epochs=150, client_data='./client_data', config='configs/cifar_Fedavg.yaml', create_client_dataset=False, cuda_visible_device=0, data='./data', dirichlet_alpha=1.5, epsilon=0.1, global_epochs=3, learning_rate_decay=0.992, local_epochs=5, log_dir=None, lr=0.1, method='Fedavg', mode='dirichlet', momentum=0.9, num_classes=10, num_of_clients=100, optimizer='sgd', participation_rate=0.1, print_freq=1, project='federated_learning', seed=0, set='CIFAR10', weight_decay=0.001, workers=8)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import copy\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=str(args.cuda_visible_device)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import models\n",
    "import json\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CIFAR10_dirichlet1.5_Fedavg\n"
     ]
    }
   ],
   "source": [
    "experiment_name=args.set+\"_\"+args.mode+(str(args.dirichlet_alpha) if args.mode=='dirichlet' else \"\")+\"_\"+args.method+(\"_\"+args.additional_experiment_name if args.additional_experiment_name!='' else '')\n",
    "print(experiment_name)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "wandb: Currently logged in as: ninigapa (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.33<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">pious-sunset-38</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/ninigapa/federated_learning\" target=\"_blank\">https://wandb.ai/ninigapa/federated_learning</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/ninigapa/federated_learning/runs/gmli8zye\" target=\"_blank\">https://wandb.ai/ninigapa/federated_learning/runs/gmli8zye</a><br/>\n",
       "                Run data is saved locally in <code>/home/dana/sonic/CIFAR/skew1class/Federated_learning/Activation_regularization/L2/wandb/run-20210706_210530-gmli8zye</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Calling run.save without any arguments is deprecated.Changes to attributes are automatically persisted.\n"
     ]
    }
   ],
   "source": [
    "wandb.init(project=args.project,group=args.mode+(str(args.dirichlet_alpha) if args.mode=='dirichlet' else \"\"),job_type=args.method+(\"_\"+args.additional_experiment_name if args.additional_experiment_name!='' else ''))\n",
    "wandb.run.name=experiment_name\n",
    "wandb.run.save()\n",
    "wandb.config.update(args)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed=args.seed\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed_all(random_seed) # if use multi-GPU\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(random_seed)\n",
    "random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "if args.set=='CIFAR10':\n",
    "    \n",
    "    \n",
    "    \n",
    "    transform_train = transforms.Compose(\n",
    "        [transforms.RandomRotation(10),\n",
    "         transforms.RandomCrop(32, padding=4),\n",
    "         transforms.RandomHorizontalFlip(),\n",
    "         transforms.ToTensor(),\n",
    "         transforms.Normalize((0.4914,0.4822,0.4465), (0.2470,0.2435,0.2616))])    \n",
    "    transform_test = transforms.Compose(\n",
    "        [\n",
    "         transforms.ToTensor(),\n",
    "         transforms.Normalize((0.4914,0.4822,0.4465), (0.2470,0.2435,0.2616))])    \n",
    "    \n",
    "    trainset = torchvision.datasets.CIFAR10(root=args.data, train=True,\n",
    "                                            download=True, transform=transform_train)\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=args.batch_size,\n",
    "                                              shuffle=True, num_workers=args.workers)\n",
    "\n",
    "    testset = torchvision.datasets.CIFAR10(root=args.data, train=False,\n",
    "                                           download=True, transform=transform_test)\n",
    "    testloader = torch.utils.data.DataLoader(testset, batch_size=args.batch_size,\n",
    "                                             shuffle=False, num_workers=args.workers)\n",
    "elif args.set=='MNIST':\n",
    "    #!wget www.di.ens.fr/~lelarge/MNIST.tar.gz\n",
    "    #!tar -zxvf MNIST.tar.gz\n",
    "\n",
    "    transform = transforms.Compose(\n",
    "        [\n",
    "         #transforms.RandomHorizontalFlip(),\n",
    "         #transforms.RandomVerticalFlip(),\n",
    "         #transforms.RandomCrop(28, padding=4),\n",
    "         transforms.ToTensor(),\n",
    "         transforms.Normalize((0.1307,), (0.3081,))\n",
    "         ])   \n",
    "    trainset = datasets.MNIST(root=args.data, train=True,\n",
    "                                            download=True,\n",
    "                                   transform=transform)\n",
    "    \n",
    "    trainloader=torch.utils.data.DataLoader(trainset, batch_size=args.batch_size,\n",
    "                                              shuffle=True, num_workers=args.workers)    \n",
    "    testset=datasets.MNIST(root=args.data, train=False,\n",
    "                                           download=True,\n",
    "                                   transform=transform)\n",
    "\n",
    "    testloader=torch.utils.data.DataLoader(testset, batch_size=args.batch_size,\n",
    "                                              shuffle=True, num_workers=args.workers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to show an image\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    #print(npimg)\n",
    "    print(np.transpose(npimg, (1, 2, 0)).shape)\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    #plt.imshow(np.transpose(npimg))#, (1, 2, 0)))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetSplit(Dataset):\n",
    "    \"\"\"An abstract Dataset class wrapped around Pytorch Dataset class.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset, idxs):\n",
    "        self.dataset = dataset\n",
    "        self.idxs = [int(i) for i in idxs]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idxs)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        image, label = self.dataset[self.idxs[item]]\n",
    "        return torch.tensor(image), torch.tensor(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cifar_iid(dataset, num_users):\n",
    "    \"\"\"\n",
    "    Sample I.I.D. client data from CIFAR10 dataset\n",
    "    :param dataset:\n",
    "    :param num_users:\n",
    "    :return: dict of image index\n",
    "    \"\"\"\n",
    "    num_items = int(len(dataset)/num_users)\n",
    "    #num_items=8\n",
    "    dict_users, all_idxs = {}, [i for i in range(len(dataset))]\n",
    "    for i in range(num_users):\n",
    "        dict_users[i] = set(np.random.choice(all_idxs, num_items, replace=False))\n",
    "        all_idxs = list(set(all_idxs) - dict_users[i])\n",
    "    return dict_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cifar_noniid(dataset, num_users):\n",
    "    \"\"\"\n",
    "    Sample non-I.I.D client data from CIFAR10 dataset\n",
    "    :param dataset:\n",
    "    :param num_users:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    #num_shards, num_imgs = 200, 250\n",
    "    #num_shards, num_imgs = 200, 250\n",
    "    class_per_user=1\n",
    "    num_shards=num_users*class_per_user\n",
    "    num_imgs=int(len(dataset)/num_shards)\n",
    "    idx_shard = [i for i in range(num_shards)]\n",
    "    dict_users = {i: np.array([],dtype='int64') for i in range(num_users)}\n",
    "    idxs = np.arange(num_shards*num_imgs)\n",
    "    # labels = dataset.train_labels.numpy()\n",
    "    #labels = np.array(dataset.train_labels)\n",
    "\n",
    "    labels=[]\n",
    "    for element in dataset:\n",
    "        labels.append(int(element[1]))\n",
    "    #print(type(labels[0]))\n",
    "    labels=np.array(labels)\n",
    "    #labels=labels.astype('int64')\n",
    "    # sort labels\n",
    "    \n",
    "    idxs_labels = np.vstack((idxs, labels))\n",
    "    idxs_labels = idxs_labels[:, idxs_labels[1, :].argsort()]\n",
    "    idxs = idxs_labels[0, :]\n",
    "\n",
    "    # divide and assign\n",
    "    for i in range(num_users):\n",
    "        rand_set = set(np.random.choice(idx_shard, class_per_user, replace=False))\n",
    "        idx_shard = list(set(idx_shard) - rand_set)\n",
    "        for rand in rand_set:\n",
    "            dict_users[i] = set(np.concatenate(\n",
    "                (dict_users[i], idxs[rand*num_imgs:(rand+1)*num_imgs]), axis=0))\n",
    "    return dict_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cifar_dirichlet(dataset, n_nets, alpha=0.5):\n",
    "    '''\n",
    "    if dataset == 'mnist':\n",
    "        X_train, y_train, X_test, y_test = load_mnist_data(datadir)\n",
    "    elif dataset == 'cifar10':\n",
    "        X_train, y_train, X_test, y_test = load_cifar10_data(datadir)\n",
    "    '''\n",
    "    #X_train=dataset[:][0]\n",
    "    y_train=torch.zeros(len(dataset),dtype=torch.long)\n",
    "    print(y_train.dtype)\n",
    "    for a in range(len(dataset)):\n",
    "        y_train[a]=(dataset[a][1])\n",
    "    n_train = len(dataset)\n",
    "    #X_train.shape[0]\n",
    "    '''\n",
    "    if partition == \"homo\":\n",
    "        idxs = np.random.permutation(n_train)\n",
    "        batch_idxs = np.array_split(idxs, n_nets)\n",
    "        net_dataidx_map = {i: batch_idxs[i] for i in range(n_nets)}\n",
    "    '''\n",
    "    #elif partition == \"hetero-dir\":\n",
    "    min_size = 0\n",
    "    K = 10\n",
    "    N=len(dataset)\n",
    "    N = y_train.shape[0]\n",
    "    net_dataidx_map = {i: np.array([],dtype='int64') for i in range(n_nets)}\n",
    "\n",
    "    while min_size < 10:\n",
    "        idx_batch = [[] for _ in range(n_nets)]\n",
    "        for k in range(K):\n",
    "            idx_k = np.where(y_train == k)[0]\n",
    "            np.random.shuffle(idx_k)\n",
    "            proportions = np.random.dirichlet(np.repeat(alpha, n_nets))\n",
    "            ## Balance\n",
    "            proportions = np.array([p*(len(idx_j)<N/n_nets) for p,idx_j in zip(proportions,idx_batch)])\n",
    "            proportions = proportions/proportions.sum()\n",
    "            proportions = (np.cumsum(proportions)*len(idx_k)).astype(int)[:-1]\n",
    "            idx_batch = [idx_j + idx.tolist() for idx_j,idx in zip(idx_batch,np.split(idx_k,proportions))]\n",
    "            min_size = min([len(idx_j) for idx_j in idx_batch])\n",
    "\n",
    "        for j in range(n_nets):\n",
    "            np.random.shuffle(idx_batch[j])\n",
    "            net_dataidx_map[j] = idx_batch[j]\n",
    "\n",
    "    #traindata_cls_counts = record_net_data_stats(y_train, net_dataidx_map, logdir)\n",
    "    return net_dataidx_map\n",
    "    #return (X_train, y_train, X_test, y_test, net_dataidx_map, traindata_cls_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(args):\n",
    "\n",
    "    print(\"=> Creating model '{}'\".format(args.arch))\n",
    "    model = models.__dict__[args.arch]()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer():\n",
    "    if args.set=='CIFAR10':\n",
    "        optimizer = optim.SGD(net.parameters(), lr=args.lr,momentum=args.momentum,weight_decay=args.weight_decay)\n",
    "    elif args.set==\"MNIST\":\n",
    "        optimizer = optim.SGD(net.parameters(), lr=args.lr,momentum=args.momentum,weight_decay=args.weight_decay)\n",
    "    else:\n",
    "        print(\"Invalid mode\")\n",
    "        return\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scheduler(optimizer):\n",
    "    if args.set=='CIFAR10':\n",
    "        scheduler = optim.lr_scheduler.LambdaLR(optimizer=optimizer,\n",
    "                                lr_lambda=lambda epoch: args.learning_rate_decay ** epoch,\n",
    "                                )\n",
    "    elif args.set==\"MNIST\":\n",
    "        scheduler = optim.lr_scheduler.LambdaLR(optimizer=optimizer,\n",
    "                                lr_lambda=lambda epoch: args.learning_rate_decay ** (int(epoch/50)),\n",
    "                                )\n",
    "    else:\n",
    "        print(\"Invalid mode\")\n",
    "        return\n",
    "    return scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.mode=='centralized':\n",
    "    net = get_model(args)\n",
    "    net.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = get_optimizer()\n",
    "    scheduler = get_scheduler(optimizer)\n",
    "    loss_train = []\n",
    "    acc_train=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.mode=='centralized':\n",
    "    for epoch in range(args.centralized_epochs):   # 데이터셋을 수차례 반복합니다.\n",
    "\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            # [inputs, labels]의 목록인 data로부터 입력을 받은 후;\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "            #print(labels)\n",
    "\n",
    "            # 변화도(Gradient) 매개변수를 0으로 만들고\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # 순전파 + 역전파 + 최적화를 한 후\n",
    "            outputs = net(inputs)\n",
    "            #print(outputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "\n",
    "\n",
    "        if epoch%args.print_freq==0:\n",
    "            loss_train.append(loss)\n",
    "            print(f\"epoch: {epoch}\")\n",
    "            print(' Average loss {:.3f}'.format( loss))\n",
    "            for j in range(1):\n",
    "                net.eval()         \n",
    "                correct = 0\n",
    "                total = 0\n",
    "                with torch.no_grad():\n",
    "                    for data in testloader:\n",
    "                        images, labels = data[0].to(device), data[1].to(device)\n",
    "                        outputs = net(images)\n",
    "                        _, predicted = torch.max(outputs.data, 1)\n",
    "                        total += labels.size(0)\n",
    "                        correct += (predicted == labels).sum().item()\n",
    "\n",
    "                print('Accuracy of the network on the 10000 test images: %f %%' % (\n",
    "                    100 * correct / total))\n",
    "            acc_train.append(100 * correct / total)\n",
    "\n",
    "            net.train()\n",
    "        scheduler.step()\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.mode=='centralized':\n",
    "    fig,ax1=plt.subplots()\n",
    "    ax2=ax1.twinx()\n",
    "    line1=ax1.plot(np.array(loss_train),color='g',label='loss_train')\n",
    "    line2=ax2.plot([i*1 for i in range(len(acc_train))],acc_train,label='acc_train')\n",
    "    lines=line1+line2\n",
    "    ax1.set_ylabel(\"loss\")\n",
    "    ax2.set_ylabel(\"accuracy\")\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.title('Experiment Result')\n",
    "    plt.legend(lines,['loss_train','acc_train'])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.mode=='centralized':\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data[0].to(device), data[1].to(device)\n",
    "            outputs = net(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Accuracy of the network on the 10000 test images: %f %%' % (\n",
    "        100 * correct / total))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LocalUpdate(object):\n",
    "    def __init__(self, lr,local_epoch,device,batch_size, dataset=None, idxs=None,alpha=0.0001):\n",
    "        self.lr=lr\n",
    "        self.local_epoch=local_epoch\n",
    "        self.device=device\n",
    "        self.loss_func = nn.CrossEntropyLoss()\n",
    "        self.selected_clients = []\n",
    "        self.ldr_train = DataLoader(DatasetSplit(dataset, idxs), batch_size=batch_size, shuffle=True)\n",
    "        self.alpha=alpha\n",
    "\n",
    "    def train(self, net):\n",
    "        net.sync_online_and_global()\n",
    "        net.train()\n",
    "        # train and update\n",
    "        max_norm = 5\n",
    "        optimizer = optim.SGD(net.parameters(), lr=self.lr,momentum=args.momentum,weight_decay=args.weight_decay)\n",
    "        epoch_loss = []\n",
    "        for iter in range(self.local_epoch):\n",
    "            batch_loss = []\n",
    "            for batch_idx, (images, labels) in enumerate(self.ldr_train):\n",
    "                images, labels = images.to(self.device), labels.to(self.device)\n",
    "                net.zero_grad()\n",
    "                log_probs,activation_l2 = net(images,online_target=True)\n",
    "                loss = self.loss_func(log_probs, labels)+self.alpha*activation_l2\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(net.parameters(), max_norm)\n",
    "                optimizer.step()\n",
    "                batch_loss.append(loss.item())\n",
    "            epoch_loss.append(sum(batch_loss)/len(batch_loss))\n",
    "        return net.state_dict(), sum(epoch_loss) / len(epoch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(mode='iid'):\n",
    "    directory=args.client_data+'/'+args.set+'/'+mode+(str(args.dirichlet_alpha) if mode=='dirichlet' else '')+'.txt'\n",
    "    check_already_exist=os.path.isfile(directory) and (os.stat(directory).st_size != 0)\n",
    "    create_new_client_data=not check_already_exist or args.create_client_dataset\n",
    "    print(\"create new client data: \"+str(create_new_client_data))\n",
    "    \n",
    "    if create_new_client_data==False:\n",
    "        try:\n",
    "            dataset={}\n",
    "            with open(directory) as f:\n",
    "                for idx,line in enumerate(f):\n",
    "                    dataset=eval(line)\n",
    "        except:\n",
    "            print(\"Have problem to read client data\")\n",
    "        \n",
    "    \n",
    "    if create_new_client_data==True:\n",
    "        if mode=='iid':\n",
    "            dataset=cifar_iid(trainset, args.num_of_clients)\n",
    "        elif mode=='skew1class':\n",
    "            dataset=cifar_noniid(trainset, args.num_of_clients)\n",
    "        elif mode=='dirichlet':\n",
    "            dataset=cifar_dirichlet(trainset, args.num_of_clients,alpha=args.dirichlet_alpha)\n",
    "        else:\n",
    "            print(\"Invalid mode ==> please select in iid, skew1class, dirichlet\")\n",
    "            return\n",
    "        try:\n",
    "            os.makedirs(args.client_data+'/'+args.set,exist_ok=True)\n",
    "            with open(directory, 'w') as f:\n",
    "                print(dataset, file=f)\n",
    "            \n",
    "        except:\n",
    "            print(\"Fail to write client data at \"+directory)\n",
    "        \n",
    "    return dataset\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_federated_learning(mode='iid'):\n",
    "    FedAvg_model=get_model(args)\n",
    "    FedAvg_model.to(device)\n",
    "    wandb.watch(FedAvg_model)\n",
    "    criterion= nn.CrossEntropyLoss().to(device)\n",
    "    criterion= nn.CrossEntropyLoss().to(device)\n",
    "    FedAvg_model.train()\n",
    "    epoch_loss = []\n",
    "    weight_saved=FedAvg_model.state_dict()\n",
    "    \n",
    "    dataset=get_dataset(mode)\n",
    "    loss_train = []\n",
    "    acc_train=[]\n",
    "    this_lr=args.lr\n",
    "    this_alpha=args.alpha\n",
    "    \n",
    "    \n",
    "    for epoch in range(args.global_epochs):\n",
    "        local_weight=[]\n",
    "        local_loss=[]\n",
    "        m=max(int(args.participation_rate*args.num_of_clients),1)\n",
    "        selected_user=np.random.choice(range(args.num_of_clients),m,replace=False)\n",
    "        print(f\"This is global {epoch} epoch\")\n",
    "        for user in selected_user:\n",
    "            local_setting=LocalUpdate(lr=this_lr,local_epoch=args.local_epochs,device=device,batch_size=args.batch_size,dataset=trainset,idxs=dataset[user],alpha=this_alpha)\n",
    "            weight,loss=local_setting.train(net=copy.deepcopy(FedAvg_model).to(device))\n",
    "            local_weight.append(copy.deepcopy(weight))\n",
    "            local_loss.append(copy.deepcopy(loss))\n",
    "        FedAvg_weight=copy.deepcopy(local_weight[0])\n",
    "        for key in FedAvg_weight.keys():\n",
    "            for i in range(1,len(local_weight)):\n",
    "                FedAvg_weight[key]+=local_weight[i][key]\n",
    "            FedAvg_weight[key]/=len(local_weight)\n",
    "        FedAvg_model.load_state_dict(FedAvg_weight)\n",
    "        loss_avg = sum(local_loss) / len(local_loss)\n",
    "        print(' Average loss {:.3f}'.format( loss_avg))\n",
    "        loss_train.append(loss_avg)\n",
    "        if epoch%args.print_freq==0:\n",
    "            FedAvg_model.eval()         \n",
    "            correct = 0\n",
    "            total = 0\n",
    "            with torch.no_grad():\n",
    "                for data in testloader:\n",
    "                    images, labels = data[0].to(device), data[1].to(device)\n",
    "                    outputs = FedAvg_model(images)\n",
    "                    _, predicted = torch.max(outputs.data, 1)\n",
    "                    total += labels.size(0)\n",
    "                    correct += (predicted == labels).sum().item()\n",
    "\n",
    "            print('Accuracy of the network on the 10000 test images: %f %%' % (\n",
    "                100 * correct / float(total)))\n",
    "            acc_train.append(100 * correct / float(total))\n",
    "\n",
    "        FedAvg_model.train()\n",
    "        \n",
    "        \n",
    "        wandb.log({mode+'_loss':loss_avg,mode+\"_acc\":acc_train[-1]})\n",
    "        \n",
    "        this_lr*=args.learning_rate_decay\n",
    "        if args.alpha_mul_epoch==True:\n",
    "            this_alpha=args.alpha*(epoch+1)\n",
    "        elif args.alpha_divide_epoch==True:\n",
    "            this_alpha=args.alpha/(epoch+1)\n",
    "            \n",
    "    fig,ax1=plt.subplots()\n",
    "    ax2=ax1.twinx()\n",
    "    line1=ax1.plot(np.array(loss_train),color='g',label='loss_train')\n",
    "    line2=ax2.plot([i*1 for i in range(len(acc_train))],acc_train,label='acc_train')\n",
    "    lines=line1+line2\n",
    "    ax1.set_ylabel(\"loss\")\n",
    "    ax2.set_ylabel(\"accuracy\")\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.title('Experiment Result')\n",
    "    plt.legend(lines,['loss_train','acc_train'])\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    print('loss_train')\n",
    "    print(loss_train)\n",
    "    \n",
    "    print('acc_train')\n",
    "    print(acc_train)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Creating model 'CNN'\n",
      "==> No gradient to conv1_global.weight\n",
      "==> No gradient to conv1_global.bias\n",
      "==> No gradient to conv2_global.weight\n",
      "==> No gradient to conv2_global.bias\n",
      "==> No gradient to fc1_global.weight\n",
      "==> No gradient to fc1_global.bias\n",
      "==> No gradient to fc2_global.weight\n",
      "==> No gradient to fc2_global.bias\n",
      "==> No gradient to fc3_global.weight\n",
      "==> No gradient to fc3_global.bias\n",
      "create new client data: False\n",
      "This is global 0 epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "main.ipynb:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"cell_type\": \"code\",\n",
      "/home/dana/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Average loss 1.799\n",
      "Accuracy of the network on the 10000 test images: 10.000000 %\n",
      "This is global 1 epoch\n",
      " Average loss 1.909\n",
      "Accuracy of the network on the 10000 test images: 10.000000 %\n",
      "This is global 2 epoch\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-365f801f871b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdo_federated_learning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-26-0fb1460f781f>\u001b[0m in \u001b[0;36mdo_federated_learning\u001b[0;34m(mode)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0muser\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mselected_user\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mlocal_setting\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLocalUpdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mthis_lr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlocal_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocal_epochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0midxs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0muser\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mthis_alpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlocal_setting\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFedAvg_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m             \u001b[0mlocal_weight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0mlocal_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-c7c2353d5031>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, net)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0miter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocal_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0mbatch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mldr_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m                 \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m                 \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-257b199ae30d>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torchvision/datasets/cifar.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;31m# doing this so that it is consistent with all other datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0;31m# to return a PIL Image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mfromarray\u001b[0;34m(obj, mode)\u001b[0m\n\u001b[1;32m   2636\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mversionadded\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1.1\u001b[0m\u001b[0;36m.6\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2637\u001b[0m     \"\"\"\n\u001b[0;32m-> 2638\u001b[0;31m     \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__array_interface__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2639\u001b[0m     \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shape\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2640\u001b[0m     \u001b[0mndim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "do_federated_learning(args.mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cifar10 FedAvg iid로 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if args.federated_iid==True:\n",
    "    do_federated_learning(mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cifar10 FedAvg Non-iid(skew1class)로 학습\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if args.federated_skew1class==True:\n",
    "    do_federated_learning(mode='skew1class')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cifar10 FedAvg Non-iid(dirichlet)로 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if args.federated_dirichlet==True:\n",
    "    do_federated_learning(mode='dirichlet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
