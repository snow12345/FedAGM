{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================\n",
      "\n",
      "Net(\n",
      "  (conv1): Conv2d(1, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=320, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      ")\n",
      "=================================\n",
      "conv1\n",
      "Conv2d(1, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "=================================\n",
      "pool1\n",
      "MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "=================================\n",
      "conv2\n",
      "Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "=================================\n",
      "pool2\n",
      "MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "=================================\n",
      "fc1\n",
      "Linear(in_features=320, out_features=50, bias=True)\n",
      "=================================\n",
      "fc2\n",
      "Linear(in_features=50, out_features=10, bias=True)\n",
      "Net(\n",
      "  (conv1): Conv2d(1, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=320, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class SaveOutput:\n",
    "    def __init__(self):\n",
    "        self.outputs = []\n",
    "        \n",
    "    def __call__(self, module, module_in, module_out):\n",
    "        self.outputs.append(module_out)\n",
    "        \n",
    "    def clear(self):\n",
    "        self.outputs = []\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        # 여기에 모든 모듈을 생성해두고,\n",
    "        # 나중에 여기에서 선언해둔 이름으로 사용할 수 있습니다.\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, 5)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(10, 20, 5)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    # 순전파 함수에서는 신경망의 구조를 정의합니다.\n",
    "    # 여기에서는 단 하나의 입력만 받지만, 필요하면 더 받도록 변경하면 됩니다.\n",
    "    def forward(self, input):\n",
    "        x = self.pool1(F.relu(self.conv1(input)))\n",
    "        x = self.pool2(F.relu(self.conv2(x)))\n",
    "\n",
    "        # 모델 구조를 정의할 때는 어떤 Python 코드를 사용해도 괜찮습니다.\n",
    "        # 모든 코드는 autograd에 의해 올바르고 완벽하게 처리될 것입니다.\n",
    "        # if x.gt(0) > x.numel() / 2:\n",
    "        #      ...\n",
    "        #\n",
    "        # 심지어 반복문을 만들고 그 안에서 동일한 모듈을 재사용해도 됩니다.\n",
    "        # 모듈은 더 이상 일시적인 상태를 갖고 있지 않으므로,\n",
    "        # 순전파 단계에서 여러번 사용해도 괜찮습니다.\n",
    "        # while x.norm(2) < 10:\n",
    "        #    x = self.conv1(x)\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net()\n",
    "\n",
    "\n",
    "save_output = SaveOutput()\n",
    "hook_handles = []\n",
    "\n",
    "for name,layer in net.named_modules():\n",
    "    print('=================================')\n",
    "    print(name)\n",
    "    print(layer)\n",
    "    \n",
    "    handle = layer.register_forward_hook(save_output)\n",
    "    hook_handles.append(handle)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([-0.0232, -0.2018,  0.5397, -0.6820, -0.7194, -1.0043,  0.1066,  0.5058,\n",
      "        -0.5767,  0.8421], grad_fn=<AddBackward0>), tensor([ 0.1940,  0.2531, -0.2500, -0.1275, -0.3000], grad_fn=<AddBackward0>), tensor([0.5303], grad_fn=<AddBackward0>)]\n"
     ]
    }
   ],
   "source": [
    "print(save_output.outputs[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fc1.weight\n",
      "None\n",
      "True\n",
      "fc1.bias\n",
      "None\n",
      "True\n",
      "fc2.weight\n",
      "None\n",
      "True\n",
      "fc2.bias\n",
      "None\n",
      "True\n",
      "fc3.weight\n",
      "None\n",
      "True\n",
      "fc3.bias\n",
      "None\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "for name, param in net.named_parameters():\n",
    "    print(name)\n",
    "    print(param.grad)\n",
    "    print(param.requires_grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "freeze_parameters(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fc1.weight\n",
      "None\n",
      "False\n",
      "fc1.bias\n",
      "None\n",
      "False\n",
      "fc2.weight\n",
      "None\n",
      "False\n",
      "fc2.bias\n",
      "None\n",
      "False\n",
      "fc3.weight\n",
      "None\n",
      "False\n",
      "fc3.bias\n",
      "None\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "for name, param in net.named_parameters():\n",
    "    print(name)\n",
    "    print(param.grad)\n",
    "    print(param.requires_grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pod(\n",
    "    list_attentions_a,\n",
    "    list_attentions_b,\n",
    "    collapse_channels=\"spatial\",\n",
    "    normalize=True,):\n",
    "    loss = torch.tensor(0.).to(list_attentions_a[0].device)\n",
    "    for i, (a, b) in enumerate(zip(list_attentions_a, list_attentions_b)):\n",
    "        a = torch.pow(a, 2)\n",
    "        b = torch.pow(b, 2)\n",
    "        if collapse_channels == \"channels\":\n",
    "            a = a.sum(dim=1).view(a.shape[0], -1)  # shape of (b, w * h)\n",
    "            b = b.sum(dim=1).view(b.shape[0], -1)\n",
    "        elif collapse_channels == \"width\":\n",
    "            a = a.sum(dim=2).view(a.shape[0], -1)  # shape of (b, c * h)\n",
    "            b = b.sum(dim=2).view(b.shape[0], -1)\n",
    "        elif collapse_channels == \"height\":\n",
    "            a = a.sum(dim=3).view(a.shape[0], -1)  # shape of (b, c * w)\n",
    "            b = b.sum(dim=3).view(b.shape[0], -1)\n",
    "        elif collapse_channels == \"gap\":\n",
    "            a = F.adaptive_avg_pool2d(a, (1, 1))[..., 0, 0]\n",
    "            b = F.adaptive_avg_pool2d(b, (1, 1))[..., 0, 0]\n",
    "        elif collapse_channels == \"spatial\":\n",
    "            a_h = a.sum(dim=3).view(a.shape[0], -1)\n",
    "            b_h = b.sum(dim=3).view(b.shape[0], -1)\n",
    "            a_w = a.sum(dim=2).view(a.shape[0], -1)\n",
    "            b_w = b.sum(dim=2).view(b.shape[0], -1)\n",
    "            a = torch.cat([a_h, a_w], dim=-1)\n",
    "            b = torch.cat([b_h, b_w], dim=-1)\n",
    "        else:\n",
    "            raise ValueError(\"Unknown method to collapse: {}\".format(collapse_channels))\n",
    "\n",
    "        if normalize:\n",
    "            a = F.normalize(a, dim=1, p=2)\n",
    "            b = F.normalize(b, dim=1, p=2)\n",
    "\n",
    "        layer_loss = torch.mean(torch.frobenius_norm(a - b, dim=-1))\n",
    "        loss += layer_loss\n",
    "    return loss / len(list_attentions_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaveOutput:\n",
    "    def __init__(self):\n",
    "        self.outputs = []\n",
    "        \n",
    "    def __call__(self, module, module_in, module_out):\n",
    "        self.outputs.append(module_out)\n",
    "        \n",
    "    def clear(self):\n",
    "        self.outputs = []\n",
    "    \n",
    "    @property\n",
    "    def outputs(self):\n",
    "        return self.outputs\n",
    "    \n",
    "\n",
    "\n",
    "def freeze_parameters(model):\n",
    "    for n,m in model.named_parameters():\n",
    "        param.grad=None\n",
    "        m.requires_grad=False\n",
    "\n",
    "class global_and_online_model(nn.Module):\n",
    "    def __init__(self,args,online_model,global_model):\n",
    "        super(global_and_online_mode,self).__init__()\n",
    "        self.args=args\n",
    "        self.online_model=online_model\n",
    "        self.global_model=copy.deepcopy(global_model)\n",
    "        freeze_parameters(self.global_model)\n",
    "\n",
    "        \n",
    "        self.online_save_output = SaveOutput()\n",
    "        self.global_save_output = SaveOutput()\n",
    "    \n",
    "\n",
    "    \n",
    "        online_hook_handles=[]\n",
    "        for layer in self.online_model.modules():\n",
    "            handle = layer.register_forward_hook(self.online_save_output)\n",
    "            online_hook_handles.append(handle)\n",
    "        \n",
    "        global_hook_handles=[]\n",
    "        for layer in self.global_model.modules():\n",
    "            handle = layer.register_forward_hook(self.global_save_output)\n",
    "            global_hook_handles.append(handle)\n",
    "    \n",
    "    def \n",
    "    \n",
    "    \n",
    "    \n",
    "    def forward(self, x,online_target=False):\n",
    "        if online_target==False:\n",
    "            return self.online_model(x)\n",
    "        \n",
    "        else:\n",
    "            self.online_save_output.clear()\n",
    "            self.global_save_output.clear()\n",
    "            \n",
    "            \n",
    "            x=self.online_model(x)\n",
    "            online_outputs=self.online_save_output.outputs[:-1]\n",
    "            \n",
    "            \n",
    "            x1=copy.deepcopy(x)\n",
    "            global_outputst=self.global_save_output.outputs[:-1]\n",
    "            \n",
    "            activation_loss=pod(    list_attentions_a=online_outputs,\n",
    "                    list_attentions_b=global_outputs,\n",
    "                    collapse_channels=self.args.collapse_channels,\n",
    "                    normalize=self.args.pod_normalize,)\n",
    "            \n",
    "            return x,activation_loss\n",
    "            \n",
    "        \n",
    "        \n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
