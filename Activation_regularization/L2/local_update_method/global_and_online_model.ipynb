{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        # 여기에 모든 모듈을 생성해두고,\n",
    "        # 나중에 여기에서 선언해둔 이름으로 사용할 수 있습니다.\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(1, 5)\n",
    "        self.fc2 = nn.Linear(5, 1)\n",
    "\n",
    "    # 순전파 함수에서는 신경망의 구조를 정의합니다.\n",
    "    # 여기에서는 단 하나의 입력만 받지만, 필요하면 더 받도록 변경하면 됩니다.\n",
    "    def forward(self, x):\n",
    "\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "net=Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KD(input_p,input_q,T=1):\n",
    "    p=F.softmax((input_p/T),dim=1)\n",
    "    q=F.softmax((input_q/T),dim=1)\n",
    "    return ((p*((p/q).log())).sum())/len(input_p)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=ResNet18()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\n",
      "ResNet\n",
      "================================\n",
      "Conv2d\n",
      "================================\n",
      "GroupNorm\n",
      "================================\n",
      "Sequential\n",
      "================================\n",
      "BasicBlock\n",
      "True\n",
      "================================\n",
      "Conv2d\n",
      "================================\n",
      "GroupNorm\n",
      "================================\n",
      "Conv2d\n",
      "================================\n",
      "GroupNorm\n",
      "================================\n",
      "Sequential\n",
      "================================\n",
      "BasicBlock\n",
      "True\n",
      "================================\n",
      "Conv2d\n",
      "================================\n",
      "GroupNorm\n",
      "================================\n",
      "Conv2d\n",
      "================================\n",
      "GroupNorm\n",
      "================================\n",
      "Sequential\n",
      "================================\n",
      "Sequential\n",
      "================================\n",
      "BasicBlock\n",
      "True\n",
      "================================\n",
      "Conv2d\n",
      "================================\n",
      "GroupNorm\n",
      "================================\n",
      "Conv2d\n",
      "================================\n",
      "GroupNorm\n",
      "================================\n",
      "Sequential\n",
      "================================\n",
      "Conv2d\n",
      "================================\n",
      "GroupNorm\n",
      "================================\n",
      "BasicBlock\n",
      "True\n",
      "================================\n",
      "Conv2d\n",
      "================================\n",
      "GroupNorm\n",
      "================================\n",
      "Conv2d\n",
      "================================\n",
      "GroupNorm\n",
      "================================\n",
      "Sequential\n",
      "================================\n",
      "Sequential\n",
      "================================\n",
      "BasicBlock\n",
      "True\n",
      "================================\n",
      "Conv2d\n",
      "================================\n",
      "GroupNorm\n",
      "================================\n",
      "Conv2d\n",
      "================================\n",
      "GroupNorm\n",
      "================================\n",
      "Sequential\n",
      "================================\n",
      "Conv2d\n",
      "================================\n",
      "GroupNorm\n",
      "================================\n",
      "BasicBlock\n",
      "True\n",
      "================================\n",
      "Conv2d\n",
      "================================\n",
      "GroupNorm\n",
      "================================\n",
      "Conv2d\n",
      "================================\n",
      "GroupNorm\n",
      "================================\n",
      "Sequential\n",
      "================================\n",
      "Sequential\n",
      "================================\n",
      "BasicBlock\n",
      "True\n",
      "================================\n",
      "Conv2d\n",
      "================================\n",
      "GroupNorm\n",
      "================================\n",
      "Conv2d\n",
      "================================\n",
      "GroupNorm\n",
      "================================\n",
      "Sequential\n",
      "================================\n",
      "Conv2d\n",
      "================================\n",
      "GroupNorm\n",
      "================================\n",
      "BasicBlock\n",
      "True\n",
      "================================\n",
      "Conv2d\n",
      "================================\n",
      "GroupNorm\n",
      "================================\n",
      "Conv2d\n",
      "================================\n",
      "GroupNorm\n",
      "================================\n",
      "Sequential\n",
      "================================\n",
      "Linear\n"
     ]
    }
   ],
   "source": [
    "for layer in a.modules():\n",
    "    print('================================')\n",
    "    name=layer._get_name()\n",
    "    print(name)\n",
    "    if 'Block' in name:\n",
    "        print(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3289, 0.3389, 0.3322],\n",
      "        [0.3289, 0.3389, 0.3322]])\n"
     ]
    }
   ],
   "source": [
    "x=torch.tensor([[0.12,0.15,0.13],[0.12,0.15,0.13]])\n",
    "print(F.softmax(x,dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2000, 0.5000, 0.3000]])\n"
     ]
    }
   ],
   "source": [
    "print(x[-1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000])"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "torch.tensor([np.e]).log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0233)"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=torch.tensor([[0.2,0.5,0.3],[0.2,0.5,0.3]])\n",
    "y=torch.tensor([[0.5,0.3,0.2],[0.5,0.3,0.2]])\n",
    "KD(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2000, 0.5000, 0.3000],\n",
      "        [0.2000, 0.5000, 0.3000]]) tensor([[0.5000, 0.3000, 0.2000],\n",
      "        [0.5000, 0.3000, 0.2000]])\n"
     ]
    }
   ],
   "source": [
    "print(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-1.4097)"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crit(x,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-1.4315)"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P = torch.Tensor([[0.36, 0.48, 0.16]])\n",
    "Q = torch.Tensor([[0.333, 0.333, 0.333]])\n",
    "crit(P,Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0084)"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "KD(P,Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0086)"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "KD(Q,P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.4772)"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.kl_div(P,Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-182-c87aa7fef195>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister_kl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkl_divergence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mQ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/distributions/kl.py\u001b[0m in \u001b[0;36mkl_divergence\u001b[0;34m(p, q)\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0m_KL_MEMOIZE\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfun\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "torch.distributions.kl.register_kl(type(P), type(Q))\n",
    "torch.distributions.kl.kl_divergence(P, Q).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "s=SaveOutput()\n",
    "for layer in net.modules():\n",
    "    handle = layer.register_forward_hook(s)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.SaveOutput object at 0x7f917a6374d0>\n"
     ]
    }
   ],
   "source": [
    "x=torch.tensor([[1.0]])\n",
    "y=net(x)\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[ 1.7720,  1.3475, -0.2756, -1.2219, -0.9436]],\n",
      "       grad_fn=<AddmmBackward>), tensor([[0.3639]], grad_fn=<AddmmBackward>), tensor([[0.3639]], grad_fn=<ReluBackward0>)]\n"
     ]
    }
   ],
   "source": [
    "print(s.get_outputs())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "for a in s.get_outputs():\n",
    "    print(a.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pod(\n",
    "    list_attentions_a,\n",
    "    list_attentions_b,\n",
    "    collapse_channels=\"spatial\",\n",
    "    normalize=True,):\n",
    "    loss = torch.tensor(0.).to(list_attentions_a[0].device)\n",
    "    for i, (a, b) in enumerate(zip(list_attentions_a, list_attentions_b)):\n",
    "        a = torch.pow(a, 2)\n",
    "        b = torch.pow(b, 2)\n",
    "        if collapse_channels == \"channels\":\n",
    "            a = a.sum(dim=1).view(a.shape[0], -1)  # shape of (b, w * h)\n",
    "            b = b.sum(dim=1).view(b.shape[0], -1)\n",
    "        elif collapse_channels == \"width\":\n",
    "            a = a.sum(dim=2).view(a.shape[0], -1)  # shape of (b, c * h)\n",
    "            b = b.sum(dim=2).view(b.shape[0], -1)\n",
    "        elif collapse_channels == \"height\":\n",
    "            a = a.sum(dim=3).view(a.shape[0], -1)  # shape of (b, c * w)\n",
    "            b = b.sum(dim=3).view(b.shape[0], -1)\n",
    "        elif collapse_channels == \"gap\":\n",
    "            a = F.adaptive_avg_pool2d(a, (1, 1))[..., 0, 0]\n",
    "            b = F.adaptive_avg_pool2d(b, (1, 1))[..., 0, 0]\n",
    "        elif collapse_channels == \"spatial\":\n",
    "            a_h = a.sum(dim=3).view(a.shape[0], -1)\n",
    "            b_h = b.sum(dim=3).view(b.shape[0], -1)\n",
    "            a_w = a.sum(dim=2).view(a.shape[0], -1)\n",
    "            b_w = b.sum(dim=2).view(b.shape[0], -1)\n",
    "            a = torch.cat([a_h, a_w], dim=-1)\n",
    "            b = torch.cat([b_h, b_w], dim=-1)\n",
    "        elif collapse_channels == \"pixel\":\n",
    "            pass        \n",
    "        else:\n",
    "            raise ValueError(\"Unknown method to collapse: {}\".format(collapse_channels))\n",
    "\n",
    "        if normalize:\n",
    "            a = F.normalize(a, dim=1, p=2)\n",
    "            b = F.normalize(b, dim=1, p=2)\n",
    "\n",
    "        layer_loss = torch.mean(torch.frobenius_norm(a - b, dim=-1))\n",
    "        loss += layer_loss\n",
    "    return loss / len(list_attentions_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaveOutput:\n",
    "    def __init__(self):\n",
    "        self.outputs = []\n",
    "        \n",
    "    def __call__(self, module, module_in, module_out):\n",
    "        self.outputs.append(module_out)\n",
    "        \n",
    "    def clear(self):\n",
    "        self.outputs = []\n",
    "    \n",
    "    def get_outputs(self):\n",
    "        return self.outputs\n",
    "    \n",
    "\n",
    "\n",
    "def freeze_parameters(model):\n",
    "    for n,m in model.named_parameters():\n",
    "        param.grad=None\n",
    "        m.requires_grad=False\n",
    "\n",
    "class global_and_online_model(nn.Module):\n",
    "    def __init__(self,args,online_model,global_model):\n",
    "        super(global_and_online_mode,self).__init__()\n",
    "        self.args=args\n",
    "        self.online_model=online_model\n",
    "        self.global_model=copy.deepcopy(global_model)\n",
    "        freeze_parameters(self.global_model)\n",
    "\n",
    "        \n",
    "        self.online_save_output = SaveOutput()\n",
    "        self.global_save_output = SaveOutput()\n",
    "    \n",
    "\n",
    "        #현재 구현 상태: 각 Conv layer의 ouptut을 가지고 와서 그들의 distillation loss를 pod function을 이용해 구함\n",
    "        #추후에 Resnet stage 단위로 바꿀 수 있다(Podnet paper:https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123650086.pdf)\n",
    "        online_hook_handles=[]\n",
    "        for layer in self.online_model.modules():\n",
    "            layer_name=layer._get_name()\n",
    "            if self.args.regularization_unit in layer_name:\n",
    "                handle = layer.register_forward_hook(self.online_save_output)\n",
    "                online_hook_handles.append(handle)\n",
    "        \n",
    "        global_hook_handles=[]\n",
    "        for layer in self.global_model.modules():\n",
    "            layer_name=layer._get_name()\n",
    "            if self.args.regularization_unit in layer_name:\n",
    "                handle = layer.register_forward_hook(self.online_save_output)\n",
    "                online_hook_handles.append(handle)\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    def forward(self, x,online_target=False):\n",
    "        if online_target==False:\n",
    "            return self.online_model(x)\n",
    "        \n",
    "        else:\n",
    "            self.online_save_output.clear()\n",
    "            self.global_save_output.clear()\n",
    "            \n",
    "            \n",
    "            x=self.online_model(x)\n",
    "            online_outputs=self.online_save_output.outputs[:-1]\n",
    "            \n",
    "            \n",
    "            x1=copy.deepcopy(x)\n",
    "            global_outputst=self.global_save_output.outputs[:-1]\n",
    "            \n",
    "            activation_loss=pod(    list_attentions_a=online_outputs,\n",
    "                    list_attentions_b=global_outputs,\n",
    "                    collapse_channels=self.args.collapse_channels,\n",
    "                    normalize=self.args.pod_normalize,)\n",
    "            \n",
    "            return x,activation_loss\n",
    "            \n",
    "        \n",
    "        \n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
